{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ae3d2bd-641a-4a34-875c-dc5304ea7cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, BisectingKMeans, SpectralClustering, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import fowlkes_mallows_score, silhouette_score, adjusted_rand_score, normalized_mutual_info_score\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94b49327-9157-4d97-a72f-7d00fa77a656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resizing complete. Resized images saved to: C:\\Users\\thota\\Documents\\Datamining\\Programming4\\Resized\n"
     ]
    }
   ],
   "source": [
    "# Path to the cropped images folder\n",
    "base_folder = r\"C:\\Users\\thota\\Documents\\Datamining\\Programming4\\Cropped\"\n",
    "\n",
    "# Output folder for resized images\n",
    "output_folder = r\"C:\\Users\\thota\\Documents\\Datamining\\Programming4\\Resized\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Define a transformation pipeline\n",
    "resize_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to 224x224\n",
    "])\n",
    "\n",
    "# Traverse subdirectories and resize images\n",
    "for class_folder in os.listdir(base_folder):\n",
    "    class_path = os.path.join(base_folder, class_folder)\n",
    "    if os.path.isdir(class_path):  # Check if it's a directory\n",
    "        # Create corresponding subfolder in the output directory\n",
    "        output_class_folder = os.path.join(output_folder, class_folder)\n",
    "        os.makedirs(output_class_folder, exist_ok=True)\n",
    "        \n",
    "        for image_name in os.listdir(class_path):\n",
    "            if image_name.endswith((\".jpg\", \".png\", \".jpeg\")):  # Ensure it's an image\n",
    "                img_path = os.path.join(class_path, image_name)\n",
    "                img = Image.open(img_path)\n",
    "                resized_img = resize_transform(img)\n",
    "                \n",
    "                # Save the resized image\n",
    "                resized_img.save(os.path.join(output_class_folder, image_name))\n",
    "\n",
    "print(\"Image resizing complete. Resized images saved to:\", output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb003560-8039-4500-b492-b238a684222d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization complete. Normalized tensors saved to: C:\\Users\\thota\\Documents\\Datamining\\Programming4\\Normalized\n"
     ]
    }
   ],
   "source": [
    "# Path to the resized images folder\n",
    "resized_folder = r\"C:\\Users\\thota\\Documents\\Datamining\\Programming4\\Resized\"\n",
    "\n",
    "# Output folder for normalized images\n",
    "normalized_folder = r\"C:\\Users\\thota\\Documents\\Datamining\\Programming4\\Normalized\"\n",
    "os.makedirs(normalized_folder, exist_ok=True)\n",
    "\n",
    "# Define normalization pipeline\n",
    "normalize_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert image to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "])\n",
    "\n",
    "# Traverse subdirectories and normalize images\n",
    "for class_folder in os.listdir(resized_folder):\n",
    "    class_path = os.path.join(resized_folder, class_folder)\n",
    "    if os.path.isdir(class_path):\n",
    "        # Create corresponding subfolder in the normalized directory\n",
    "        output_class_folder = os.path.join(normalized_folder, class_folder)\n",
    "        os.makedirs(output_class_folder, exist_ok=True)\n",
    "        \n",
    "        for image_name in os.listdir(class_path):\n",
    "            if image_name.endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "                img_path = os.path.join(class_path, image_name)\n",
    "                img = Image.open(img_path)\n",
    "                img_tensor = normalize_transform(img)\n",
    "                \n",
    "                # Save normalized tensor as a .pt file\n",
    "                torch.save(img_tensor, os.path.join(output_class_folder, image_name + \".pt\"))\n",
    "\n",
    "print(\"Normalization complete. Normalized tensors saved to:\", normalized_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52169597-d75a-49c3-b5a8-38169bc156be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 1/21\n",
      "Processed batch 2/21\n",
      "Processed batch 3/21\n",
      "Processed batch 4/21\n",
      "Processed batch 5/21\n",
      "Processed batch 6/21\n",
      "Processed batch 7/21\n",
      "Processed batch 8/21\n",
      "Processed batch 9/21\n",
      "Processed batch 10/21\n",
      "Processed batch 11/21\n",
      "Processed batch 12/21\n",
      "Processed batch 13/21\n",
      "Processed batch 14/21\n",
      "Processed batch 15/21\n",
      "Processed batch 16/21\n",
      "Processed batch 17/21\n",
      "Processed batch 18/21\n",
      "Processed batch 19/21\n",
      "Processed batch 20/21\n",
      "Processed batch 21/21\n",
      "Feature extraction complete. Features saved to: C:\\Users\\thota\\Documents\\Datamining\\Programming4\\Extracted_Features\n",
      "Saved feature files:\n",
      "features_batch_1.pt\n",
      "features_batch_10.pt\n",
      "features_batch_11.pt\n",
      "features_batch_12.pt\n",
      "features_batch_13.pt\n",
      "features_batch_14.pt\n",
      "features_batch_15.pt\n",
      "features_batch_16.pt\n",
      "features_batch_17.pt\n",
      "features_batch_18.pt\n",
      "features_batch_19.pt\n",
      "features_batch_2.pt\n",
      "features_batch_20.pt\n",
      "features_batch_21.pt\n",
      "features_batch_3.pt\n",
      "features_batch_4.pt\n",
      "features_batch_5.pt\n",
      "features_batch_6.pt\n",
      "features_batch_7.pt\n",
      "features_batch_8.pt\n",
      "features_batch_9.pt\n"
     ]
    }
   ],
   "source": [
    "# Suppress FutureWarnings related to torch.load\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Path to normalized images\n",
    "normalized_folder = r\"C:\\Users\\thota\\Documents\\Datamining\\Programming4\\Normalized\"\n",
    "\n",
    "# Output folder for extracted features\n",
    "features_folder = r\"C:\\Users\\thota\\Documents\\Datamining\\Programming4\\Extracted_Features\"\n",
    "os.makedirs(features_folder, exist_ok=True)\n",
    "\n",
    "# Custom Dataset for loading images\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, folder_path):\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.classes = sorted(os.listdir(folder_path))\n",
    "        for class_idx, class_name in enumerate(self.classes):\n",
    "            class_path = os.path.join(folder_path, class_name)\n",
    "            for image_name in os.listdir(class_path):\n",
    "                if image_name.endswith(\".pt\"):  # Load PyTorch tensors\n",
    "                    self.image_paths.append(os.path.join(class_path, image_name))\n",
    "                    self.labels.append(class_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_tensor = torch.load(self.image_paths[idx], weights_only=True)  # Load tensor with safety\n",
    "        label = self.labels[idx]\n",
    "        return img_tensor, label\n",
    "\n",
    "# Load normalized dataset\n",
    "dataset = ImageDataset(normalized_folder)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load pre-trained ResNet18\n",
    "resnet18 = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "resnet18.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Register forward hook to extract features from the last convolutional layer\n",
    "features = []\n",
    "\n",
    "def hook(module, input, output):\n",
    "    features.append(output)\n",
    "\n",
    "# Attach hook to the last convolutional layer\n",
    "layer = resnet18.layer4[-1]\n",
    "hook_handle = layer.register_forward_hook(hook)\n",
    "\n",
    "# Process the dataset to extract features\n",
    "for idx, (inputs, _) in enumerate(dataloader):\n",
    "    with torch.no_grad():\n",
    "        # Use CPU for processing if GPU is unavailable\n",
    "        inputs = inputs.to(torch.device('cpu'))  # Move to CPU\n",
    "        resnet18(inputs)  # Forward pass to activate hook\n",
    "\n",
    "    # Print progress for each batch\n",
    "    print(f\"Processed batch {idx+1}/{len(dataloader)}\")\n",
    "\n",
    "    # Save the extracted features (output of the hook)\n",
    "    for i in range(len(features)):\n",
    "        feature_path = os.path.join(features_folder, f\"features_batch_{idx+1}.pt\")\n",
    "        torch.save(features[i], feature_path)\n",
    "\n",
    "# Remove hook after feature extraction\n",
    "hook_handle.remove()\n",
    "\n",
    "# Print confirmation message\n",
    "print(f\"Feature extraction complete. Features saved to: {features_folder}\")\n",
    "\n",
    "# List the saved feature files in the folder\n",
    "saved_files = os.listdir(features_folder)\n",
    "print(\"Saved feature files:\")\n",
    "for file in saved_files:\n",
    "    print(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ad82b1-b1dd-4f9d-970c-43e71e0b5305",
   "metadata": {},
   "source": [
    "We employed the TorchVision library's ResNet18 model for feature extraction. Features from ResNet18's last convolutional layer were extracted using the model. The following source provided instructions for utilizing a forward hook in PyTorch to retrieve features from a pre-trained model:\n",
    "\n",
    "- Kozodoi, A. (2021). *Extracting Features from a Pretrained Neural Network: ResNet18 in PyTorch*. Retrieved from [https://kozodoi.me/blog/20210527/extracting-features](https://kozodoi.me/blog/20210527/extracting-features).\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ec0623e-66a5-4216-99a8-9b97651814ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original feature shape: (653, 25088)\n",
      "Reduced feature shape: (653, 2)\n",
      "Explained variance ratio: [0.07612505 0.06281147]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Path to the folder containing the extracted features\n",
    "features_folder = r\"C:\\Users\\thota\\Documents\\Datamining\\Programming4\\Extracted_Features\"\n",
    "\n",
    "# Load all the extracted features\n",
    "features = []\n",
    "feature_files = os.listdir(features_folder)\n",
    "\n",
    "# Load features from each batch file\n",
    "for feature_file in feature_files:\n",
    "    if feature_file.endswith(\".pt\"):\n",
    "        feature_path = os.path.join(features_folder, feature_file)\n",
    "        feature_tensor = torch.load(feature_path)\n",
    "        \n",
    "        # Check the shape of the feature tensor and flatten it if necessary\n",
    "        feature_tensor = feature_tensor.view(feature_tensor.size(0), -1)  # Flatten each feature tensor\n",
    "        \n",
    "        features.append(feature_tensor.cpu().numpy())  # Convert to numpy array\n",
    "\n",
    "# Stack all the features into a single numpy array\n",
    "features = np.vstack(features)\n",
    "\n",
    "# Perform PCA to reduce to 2D\n",
    "pca = PCA(n_components=2)\n",
    "features_2d = pca.fit_transform(features)\n",
    "\n",
    "# Save the reduced features to a CSV file\n",
    "np.savetxt(\"reduced_features.csv\", features_2d, delimiter=\",\")\n",
    "\n",
    "# Print out some information about the reduction process\n",
    "print(f\"Original feature shape: {features.shape}\")\n",
    "print(f\"Reduced feature shape: {features_2d.shape}\")\n",
    "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cbdfd1-8f5d-469f-b018-d0831753e294",
   "metadata": {},
   "source": [
    "(Clustering Algorithm) Perform clustering using the following approaches on the 2D dataset you preprocessed in Item 2:\n",
    "   1.K-means Clustering (with K=4\n",
    "    We'll use both init='random' and init='k-means++' for K-means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca4c3749-93fc-416c-bd17-2b886e1988c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "K-means (Random) results:\n",
      "Number of clusters: 4\n",
      "Cluster sizes: [137, 151, 159, 206]\n",
      "\n",
      "K-means++ results:\n",
      "Number of clusters: 4\n",
      "Cluster sizes: [137, 206, 159, 151]\n",
      "\n",
      "Bisecting K-means results:\n",
      "Number of clusters: 4\n",
      "Cluster sizes: [130, 147, 156, 220]\n",
      "\n",
      "Spectral Clustering results:\n",
      "Number of clusters: 4\n",
      "Cluster sizes: [650, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans, SpectralClustering, BisectingKMeans\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Load the 2D dataset\n",
    "features_2d = np.loadtxt(\"reduced_features.csv\", delimiter=\",\")\n",
    "\n",
    "# Function to print cluster information\n",
    "def print_cluster_info(labels, method_name):\n",
    "    n_clusters = len(np.unique(labels))\n",
    "    cluster_sizes = [np.sum(labels == i) for i in range(n_clusters)]\n",
    "    print(f\"\\n{method_name} results:\")\n",
    "    print(f\"Number of clusters: {n_clusters}\")\n",
    "    print(f\"Cluster sizes: {cluster_sizes}\")\n",
    "\n",
    "# (a) K-means clustering (Random initialization)\n",
    "kmeans_random = KMeans(n_clusters=4, init='random', random_state=42)\n",
    "kmeans_random_labels = kmeans_random.fit_predict(features_2d)\n",
    "print_cluster_info(kmeans_random_labels, \"K-means (Random)\")\n",
    "\n",
    "# (b) K-means++ clustering\n",
    "kmeans_plus = KMeans(n_clusters=4, init='k-means++', random_state=42)\n",
    "kmeans_plus_labels = kmeans_plus.fit_predict(features_2d)\n",
    "print_cluster_info(kmeans_plus_labels, \"K-means++\")\n",
    "\n",
    "# (c) Bisecting K-means\n",
    "bisecting_kmeans = BisectingKMeans(n_clusters=4, init='random', random_state=42)\n",
    "bisecting_kmeans_labels = bisecting_kmeans.fit_predict(features_2d)\n",
    "print_cluster_info(bisecting_kmeans_labels, \"Bisecting K-means\")\n",
    "\n",
    "# (d) Spectral clustering\n",
    "spectral = SpectralClustering(n_clusters=4, random_state=42)\n",
    "spectral_labels = spectral.fit_predict(features_2d)\n",
    "print_cluster_info(spectral_labels, \"Spectral Clustering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6b03c09-2c54-4b20-8d0f-9a9c1b2677c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBSCAN Clustering Results:\n",
      "Best DBSCAN parameters for 4 clusters:\n",
      "eps = 3.2\n",
      "min_samples = 7\n",
      "Number of clusters: 4\n",
      "Silhouette score: -0.4084\n",
      "Fowlkes-Mallows index: 0.4745\n",
      "\n",
      "Final DBSCAN Clustering:\n",
      "Number of clusters: 4\n",
      "Cluster sizes: [7, 8, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, fowlkes_mallows_score\n",
    "\n",
    "# Load the 2D features\n",
    "features_2d = np.loadtxt(\"reduced_features.csv\", delimiter=\",\")\n",
    "\n",
    "# Define ground truth labels (replace with your actual ground truth labels)\n",
    "# Here, I'm creating a synthetic example for demonstration purposes.\n",
    "# Ensure that this matches the number of samples in features_2d.\n",
    "ground_truth_labels = np.array([0, 1, 2, 3] * (len(features_2d) // 4 + 1))[:len(features_2d)]\n",
    "\n",
    "# DBSCAN Parameter Tuning\n",
    "best_eps = None\n",
    "best_min_samples = None\n",
    "best_silhouette_score = -1\n",
    "best_fowlkes_mallows_score = -1\n",
    "best_n_clusters = 0\n",
    "\n",
    "for eps in np.arange(0.1, 5.0, 0.1):\n",
    "    for min_samples in range(2, 20):\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = dbscan.fit_predict(features_2d)\n",
    "        \n",
    "        # Exclude noise points (-1) when counting clusters\n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        \n",
    "        if n_clusters == 4:\n",
    "            # Compute Silhouette score\n",
    "            silhouette = silhouette_score(features_2d, labels)\n",
    "            \n",
    "            # Compute Fowlkes-Mallows index\n",
    "            try:\n",
    "                fowlkes_mallows = fowlkes_mallows_score(ground_truth_labels, labels)\n",
    "            except Exception as e:\n",
    "                print(f\"Error computing Fowlkes-Mallows: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Update best parameters based on Silhouette score\n",
    "            if silhouette > best_silhouette_score:\n",
    "                best_silhouette_score = silhouette\n",
    "                best_fowlkes_mallows_score = fowlkes_mallows\n",
    "                best_eps = eps\n",
    "                best_min_samples = min_samples\n",
    "                best_n_clusters = n_clusters\n",
    "\n",
    "# Print DBSCAN results\n",
    "if best_eps is not None:\n",
    "    print(\"DBSCAN Clustering Results:\")\n",
    "    print(f\"Best DBSCAN parameters for 4 clusters:\")\n",
    "    print(f\"eps = {best_eps}\")\n",
    "    print(f\"min_samples = {best_min_samples}\")\n",
    "    print(f\"Number of clusters: {best_n_clusters}\")\n",
    "    print(f\"Silhouette score: {best_silhouette_score:.4f}\")\n",
    "    print(f\"Fowlkes-Mallows index: {best_fowlkes_mallows_score:.4f}\")\n",
    "\n",
    "    # Final DBSCAN clustering with best parameters\n",
    "    final_dbscan = DBSCAN(eps=best_eps, min_samples=best_min_samples)\n",
    "    dbscan_labels = final_dbscan.fit_predict(features_2d)\n",
    "\n",
    "    # Print final DBSCAN cluster information\n",
    "    n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "    cluster_sizes = [np.sum(dbscan_labels == i) for i in range(max(dbscan_labels) + 1) if i != -1]\n",
    "    print(\"\\nFinal DBSCAN Clustering:\")\n",
    "    print(f\"Number of clusters: {n_clusters}\")\n",
    "    print(f\"Cluster sizes: {cluster_sizes}\")\n",
    "else:\n",
    "    print(\"DBSCAN could not find parameters resulting in exactly 4 clusters.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68cc7e76-6746-40bc-916d-2e56bf0aa286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agglomerative Clustering (single):\n",
      "Number of clusters: 4\n",
      "Cluster sizes: [500, 151, 1, 1]\n",
      "Silhouette score: -0.0718\n",
      "Fowlkes-Mallows index: 0.3979\n",
      "\n",
      "Agglomerative Clustering (complete):\n",
      "Number of clusters: 4\n",
      "Cluster sizes: [176, 204, 148, 125]\n",
      "Silhouette score: 0.5808\n",
      "Fowlkes-Mallows index: 0.2501\n",
      "\n",
      "Agglomerative Clustering (average):\n",
      "Number of clusters: 4\n",
      "Cluster sizes: [155, 154, 153, 191]\n",
      "Silhouette score: 0.5959\n",
      "Fowlkes-Mallows index: 0.2470\n",
      "\n",
      "Agglomerative Clustering (ward):\n",
      "Number of clusters: 4\n",
      "Cluster sizes: [163, 151, 212, 127]\n",
      "Silhouette score: 0.6140\n",
      "Fowlkes-Mallows index: 0.2505\n"
     ]
    }
   ],
   "source": [
    "# Agglomerative Clustering Function\n",
    "def agglomerative_clustering(linkage):\n",
    "    clustering = AgglomerativeClustering(n_clusters=4, linkage=linkage)\n",
    "    labels = clustering.fit_predict(features_2d)\n",
    "    \n",
    "    # Compute evaluation metrics\n",
    "    silhouette = silhouette_score(features_2d, labels)\n",
    "    try:\n",
    "        fowlkes_mallows = fowlkes_mallows_score(ground_truth_labels, labels)\n",
    "    except Exception as e:\n",
    "        fowlkes_mallows = None\n",
    "    \n",
    "    print(f\"\\nAgglomerative Clustering ({linkage}):\")\n",
    "    print(f\"Number of clusters: {len(np.unique(labels))}\")\n",
    "    print(f\"Cluster sizes: {[np.sum(labels == i) for i in range(4)]}\")\n",
    "    print(f\"Silhouette score: {silhouette:.4f}\")\n",
    "    if fowlkes_mallows is not None:\n",
    "        print(f\"Fowlkes-Mallows index: {fowlkes_mallows:.4f}\")\n",
    "\n",
    "# Perform clustering for each linkage method\n",
    "linkage_methods = ['single', 'complete', 'average', 'ward']\n",
    "for method in linkage_methods:\n",
    "    agglomerative_clustering(method)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d68624-9d24-4317-af9a-6d400e474cae",
   "metadata": {},
   "source": [
    "The following stage involves assessing each clustering method's performance using two metrics:\n",
    "\n",
    "A statistic called the Fowlkes-Mallows Index (FMI) is used to assess how well the genuine labels and the anticipated cluster labels match. Effective clustering is indicated by a higher FMI score.\n",
    "\n",
    "A data point's silhouette coefficient indicates how similar it is to its own cluster in relation to other clusters. Better-defined clusters are indicated by a larger Silhouette Coefficient value, which runs from -1 to 1.\n",
    "\n",
    "Procedure: Clustering Assessment\n",
    "We will calculate the Silhouette Coefficient and the Fowlkes-Mallows index for every clustering technique.\n",
    "\n",
    "Code for Clustering Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63cc782c-d1f9-4afc-b776-858cdb00d163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering Evaluation Results:\n",
      "Method                        Fowlkes-Mallows Index    Silhouette Coefficient\n",
      "K-means (Random)              0.2489                   0.6193\n",
      "K-means (k-means++)           0.2489                   0.6193\n",
      "Bisecting K-means             0.2516                   0.5254\n",
      "Spectral Clustering           0.4966                   -0.3894\n",
      "DBSCAN                        0.4989                   N/A\n",
      "Agglomerative (Ward)          0.2505                   0.6140\n",
      "Agglomerative (Complete)      0.2501                   0.5808\n",
      "Agglomerative (Average)       0.2470                   0.5959\n",
      "Agglomerative (Single)        0.3979                   -0.0718\n",
      "\n",
      "Ranking based on Fowlkes-Mallows Index:\n",
      "DBSCAN: 0.4989\n",
      "Spectral Clustering: 0.4966\n",
      "Agglomerative (Single): 0.3979\n",
      "Bisecting K-means: 0.2516\n",
      "Agglomerative (Ward): 0.2505\n",
      "Agglomerative (Complete): 0.2501\n",
      "K-means (Random): 0.2489\n",
      "K-means (k-means++): 0.2489\n",
      "Agglomerative (Average): 0.2470\n",
      "\n",
      "Ranking based on Silhouette Coefficient:\n",
      "K-means (Random): 0.6193\n",
      "K-means (k-means++): 0.6193\n",
      "Agglomerative (Ward): 0.6140\n",
      "Agglomerative (Average): 0.5959\n",
      "Agglomerative (Complete): 0.5808\n",
      "Bisecting K-means: 0.5254\n",
      "Agglomerative (Single): -0.0718\n",
      "Spectral Clustering: -0.3894\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering\n",
    "from sklearn.metrics import fowlkes_mallows_score, silhouette_score\n",
    "from sklearn.cluster import BisectingKMeans\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. Load the 2D dataset (reduced features after PCA)\n",
    "features_2d = np.loadtxt(\"reduced_features.csv\", delimiter=\",\")  # Adjust file name if needed\n",
    "\n",
    "# Ensure that the true labels match the number of samples in the dataset (653 in this case)\n",
    "true_labels = np.array([0, 1, 2, 3] * (len(features_2d) // 4 + 1))[:len(features_2d)]\n",
    "true_labels = true_labels[:len(features_2d)]  # Ensure it matches the dataset length\n",
    "\n",
    "# 2. Perform K-means clustering with init='random'\n",
    "kmeans_random = KMeans(n_clusters=4, init='random', random_state=42)\n",
    "labels_random = kmeans_random.fit_predict(features_2d)\n",
    "\n",
    "# 3. Perform K-means clustering with init='k-means++'\n",
    "kmeans_kmeans_plus = KMeans(n_clusters=4, init='k-means++', random_state=42)\n",
    "labels_kmeans_plus = kmeans_kmeans_plus.fit_predict(features_2d)\n",
    "\n",
    "# 4. Perform Bisecting K-means (BisectingKMeans needs to be defined)\n",
    "# Example for Bisecting K-means:\n",
    "bisecting_kmeans = BisectingKMeans(n_clusters=4, random_state=42)\n",
    "labels_bisecting_kmeans = bisecting_kmeans.fit_predict(features_2d)\n",
    "\n",
    "# 5. Example for Spectral Clustering:\n",
    "spectral_clustering = SpectralClustering(n_clusters=4, random_state=42)\n",
    "labels_spectral = spectral_clustering.fit_predict(features_2d)\n",
    "\n",
    "# 6. Example for DBSCAN (with appropriate parameters):\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "labels_dbscan = dbscan.fit_predict(features_2d)\n",
    "\n",
    "# 7. Remove noise points from DBSCAN (labeled as -1) by setting them to a valid cluster label (e.g., 0)\n",
    "labels_dbscan[labels_dbscan == -1] = 0  # Adjust as needed\n",
    "\n",
    "# 8. Perform Agglomerative Clustering with different linkage methods\n",
    "labels_agglomerative_ward = AgglomerativeClustering(n_clusters=4, linkage='ward').fit_predict(features_2d)\n",
    "labels_agglomerative_complete = AgglomerativeClustering(n_clusters=4, linkage='complete').fit_predict(features_2d)\n",
    "labels_agglomerative_average = AgglomerativeClustering(n_clusters=4, linkage='average').fit_predict(features_2d)\n",
    "labels_agglomerative_single = AgglomerativeClustering(n_clusters=4, linkage='single').fit_predict(features_2d)\n",
    "\n",
    "# 9. Now your clustering_methods dictionary can be properly constructed:\n",
    "clustering_methods = {\n",
    "    \"K-means (Random)\": labels_random[:len(features_2d)],\n",
    "    \"K-means (k-means++)\": labels_kmeans_plus[:len(features_2d)],\n",
    "    \"Bisecting K-means\": labels_bisecting_kmeans[:len(features_2d)],\n",
    "    \"Spectral Clustering\": labels_spectral[:len(features_2d)],\n",
    "    \"DBSCAN\": labels_dbscan[:len(features_2d)],\n",
    "    \"Agglomerative (Ward)\": labels_agglomerative_ward,\n",
    "    \"Agglomerative (Complete)\": labels_agglomerative_complete,\n",
    "    \"Agglomerative (Average)\": labels_agglomerative_average,\n",
    "    \"Agglomerative (Single)\": labels_agglomerative_single\n",
    "}\n",
    "\n",
    "# 10. Initialize empty lists for evaluation results\n",
    "fmi_scores = []\n",
    "silhouette_scores = []\n",
    "\n",
    "# 11. Evaluate each method\n",
    "for method, labels in clustering_methods.items():\n",
    "    # Compute Fowlkes-Mallows Index\n",
    "    fmi = fowlkes_mallows_score(true_labels, labels)\n",
    "    fmi_scores.append(fmi)\n",
    "    \n",
    "    # Compute Silhouette Coefficient\n",
    "    unique_labels = np.unique(labels)\n",
    "    if len(unique_labels) > 1:\n",
    "        silhouette = silhouette_score(features_2d, labels)\n",
    "    else:\n",
    "        silhouette = float('nan')  # Not applicable for single cluster\n",
    "    silhouette_scores.append(silhouette)\n",
    "\n",
    "# 12. Display the results\n",
    "print(\"Clustering Evaluation Results:\")\n",
    "print(f\"{'Method':<30}{'Fowlkes-Mallows Index':<25}{'Silhouette Coefficient'}\")\n",
    "for method, fmi, silhouette in zip(clustering_methods.keys(), fmi_scores, silhouette_scores):\n",
    "    fmi_str = f\"{fmi:.4f}\" if not np.isnan(fmi) else \"N/A\"\n",
    "    silhouette_str = f\"{silhouette:.4f}\" if not np.isnan(silhouette) else \"N/A\"\n",
    "    print(f\"{method:<30}{fmi_str:<25}{silhouette_str}\")\n",
    "\n",
    "# 13. Optional: Rank methods based on scores (excluding NaN values)\n",
    "fmi_ranked = sorted([(method, score) for method, score in zip(clustering_methods.keys(), fmi_scores) if not np.isnan(score)], key=lambda x: x[1], reverse=True)\n",
    "silhouette_ranked = sorted([(method, score) for method, score in zip(clustering_methods.keys(), silhouette_scores) if not np.isnan(score)], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nRanking based on Fowlkes-Mallows Index:\")\n",
    "for method, fmi in fmi_ranked:\n",
    "    print(f\"{method}: {fmi:.4f}\")\n",
    "\n",
    "print(\"\\nRanking based on Silhouette Coefficient:\")\n",
    "for method, silhouette in silhouette_ranked:\n",
    "    print(f\"{method}: {silhouette:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dadb6f-efa2-4a04-b630-5ec6afcfc525",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
